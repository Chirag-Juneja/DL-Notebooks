{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf08d38",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c036002",
   "metadata": {},
   "source": [
    "## Four branches of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee4ace",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "It consists of learning to map input data to known targets (also called annotations), given a set of examples(often annotated by humans.\n",
    "\n",
    "Examples:\n",
    "   * Optical Character Recognition.\n",
    "   * Speech Recognition.\n",
    "   * image Classification.\n",
    "   * Language Translation.\n",
    "   * Sequence Generation.\n",
    "   * Syntax tree prediction.\n",
    "   * Object detection.\n",
    "   * Image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee088871",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "This consists of finding interesting transformations of the input data without the help of any targets.\n",
    "Unsupervised learning is often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. \n",
    "Dimensionality reduction and clustering are well-known categories of unsupervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d34d02",
   "metadata": {},
   "source": [
    "### Self Supervised Learning\n",
    "\n",
    "Self supervised learning is supervised learning without human-annotated labels. It is supervised learning without human in the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20862d2",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "In reinforcement learning, an agent receives information about its environment and learns to choose actions that will maximize some reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f13146",
   "metadata": {},
   "source": [
    "## Evaluating machine learning models\n",
    "\n",
    "Evaluating a model always boils down to splitting the available data into three sets: training, validation and test. You train on th training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data.\n",
    "\n",
    "Hyperparameters are tuned on the validation set and not on the test set as it will result in overfitting.\n",
    "\n",
    "### Simple hold out validation\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set. \n",
    "\n",
    "### K fold Validation \n",
    "Split the data into K partitions of equal size. For each partition i, train a model on the remaining K-1 partitions, and evaluate it on partition i. Final score is then the average of the K scores obtained. \n",
    "\n",
    "### Data representativeness \n",
    "Both training set and test set should be representative of the data at hand.\n",
    "\n",
    "### The arrow of time\n",
    "If the model is trying to predict the future given the past, then you should not randomly shuffle the data before splitting it, because it will create a temporal leak.\n",
    "\n",
    "### Redundancy in data\n",
    "Make sure your training set and validation set are disjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da46a7",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "### Vectorization\n",
    "All input and targets in neural networks must be tensors of floating point data. Converting the input data into tensors is called data vectorization.\n",
    "\n",
    "\n",
    "### Value Normalization\n",
    "To make learning easier for the network, the data should have the following characteristics:\n",
    "* Take small values - most values should be in the range of 0-1.\n",
    "* Be homogenous - all features should take values in roughly the same range.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand ( in this case, neural network) to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before it goes into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449344ab",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "Optimization refers to the process of adjusting model to get the best preformance possible on the training data.\n",
    "Generalization refers to how well the trained model performs on data it has never seen before.\n",
    "\n",
    "At the begining of training, optimization and generalization are correlated: the lower the loss on training data, the lower the loss on test data. While this is happening, model is said to be underfit. There is still progress to be made; the network hasn't yet modeled all relevent patterns in the training data. But after a certain number of iternations on the training data, generalization stops improving, and validation metrics stall and then begin to degrade: the model is starting to overfit. That is, it's begining to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data.\n",
    "\n",
    "### Regularization\n",
    "If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patters, which have better change of generalizing well.\n",
    "The process of fighting overfitting this way is called regularization.\n",
    "\n",
    "#### Reducing the network's size\n",
    "The simpleest way to prevent overfitting is to reduce the size of the model.\n",
    "\n",
    "#### Adding weight regularization\n",
    "Weight regularization provides an approach to reduce the overfitting of a deep learning neural network model on the training data and improve the performance of the model on new data. Its done by adding to the loss function of the network a cost associated with having large weights.\n",
    "L1 regularization -  The cost added is proportional to the absolute value of the weight coefficients\n",
    "L2 regularization - The cost added is proportional to the square of the value of the weight coefficients. Its also called weight decay.\n",
    "\n",
    "#### Adding dropout\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks. Dropout applied to a layer, consists of randomly dropping out a number of output features of the layer during training. Dropout rate is generally set between 0.2-0.5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
